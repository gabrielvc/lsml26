{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc34c26",
   "metadata": {},
   "source": [
    "# Generative models: VAE + Diffusion = Latent Diffusion.\n",
    "\n",
    "The goal of this notebook is to introduce the students to VAE and Diffusion training. To do so, we will consider the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4066b6d",
   "metadata": {},
   "source": [
    "## Dataset Loading and exploration.\n",
    "\n",
    "The next cells define simple data loading procedures and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, RandomRotation, RandomHorizontalFlip, Pad\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from torch import randint, jit, nn, distributions, chunk, ones, ones_like, zeros, zeros_like, Tensor, optim, linalg, randn, randn_like, no_grad, concatenate, linspace\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "from typing import Union, Dict, Any, Tuple, Callable\n",
    "import itertools\n",
    "import math\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset, StackDataset\n",
    "from diffusers import UNet2DModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"data\"\n",
    "download = False\n",
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        jit.script(\n",
    "            nn.Sequential(\n",
    "                # RandomHorizontalFlip(),\n",
    "                # RandomRotation(45, fill=0),\n",
    "                Pad((2, 2), fill=0)\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_dataset = FashionMNIST(root_path, transform=transform, download=download)\n",
    "transform_test = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        Pad((2, 2), fill=0)\n",
    "    ]\n",
    ")\n",
    "test_dataset = FashionMNIST(root_path, transform=transform_test, train=False, download=download)\n",
    "indices_train = list(range(0, 50_000))\n",
    "indices_cv = list(range(50_000, 60_000))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(indices_train)\n",
    "cv_sampler = SubsetRandomSampler(indices_cv)\n",
    "\n",
    "###\n",
    "# !!!!ATTENTION!!!!!You might want to adapt this to your machine\n",
    "batch_size = 512\n",
    "num_workers = 4\n",
    "###\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "cv_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=cv_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "n_channels, height, width = train_dataset[0][0].shape\n",
    "train_size = 50_000\n",
    "cv_size = 10_000\n",
    "print(f\"Total of {train_size} images of shape {(n_channels, height, width)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_display = 10\n",
    "fig, axes = plt.subplots(1, n_display, figsize=(3*n_display, 3.5))\n",
    "batch_dt = next(iter(train_loader))\n",
    "for i, ax in zip(randint(0, batch_size, (n_display,)), axes):\n",
    "    ax.imshow(batch_dt[0][i, 0], vmin=0, vmax=1)\n",
    "    ax.set_title(batch_dt[1][i].item())\n",
    "    ax.set_axis_off()\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3d80f",
   "metadata": {},
   "source": [
    "## VAE Training\n",
    "\n",
    "We now focus on the training of the VAE. To do so, we will rely on [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/starter/introduction.html). Lightning is a really handy tool. It avoids writing boilerplate code and has several functions that allow to scale your code.\n",
    "It main object is the LightningModule. \n",
    "\n",
    "### Q1.1:\n",
    "In the cell below, complete both the code for the method Elbo in the AbstractVAE class.\n",
    "### Q1.2\n",
    "In the cell below, complete the class GaussGaussVAE in order to implement a model where both $p(z |x)$, $p(x|z)$ and $p(z)$ are Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c165792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_independent_gaussian(mean, logvar, img_dims=(1, 2, 3)):\n",
    "    dist = distributions.Normal(\n",
    "        loc=mean, scale=(0.5 * logvar).exp(), validate_args=False\n",
    "    )\n",
    "    dist = distributions.Independent(dist, 3, validate_args=False)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class AbstractPrior(object):\n",
    "    def rsample(shape):\n",
    "        raise NotImplementedError(\"You should pass a Prior\")\n",
    "\n",
    "\n",
    "class AbstractVAE(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_class: nn.Module,\n",
    "        decoder_class: nn.Module,\n",
    "        encoder_params: Dict[str, Any],\n",
    "        decoder_params: Dict[str, Any],\n",
    "        optim_config,\n",
    "        n_images_to_log: int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.prior = AbstractPrior()\n",
    "        self.optim_config = optim_config\n",
    "        self.encoder = encoder_class(**encoder_params)\n",
    "        self.decoder = decoder_class(**decoder_params)\n",
    "        self.n_images_to_log = n_images_to_log\n",
    "\n",
    "    def encode(self, x: Tensor) -> distributions.Distribution:\n",
    "        \"\"\"\n",
    "        The goal of this function is to return the latent distribution from a given data sample $x$ as a pytorch distribution ! In probabilistic notation, p(z |x).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Encode not implemented\")\n",
    "\n",
    "    def decode(self, latents: Tensor) -> distributions.Distribution:\n",
    "        \"\"\"\n",
    "        The goal of this function is to return the distribution of a data sample given a latent code $z$ as a pytorch distribution. In probabilistic notation, p(x | z)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Decoder not implemented\")\n",
    "\n",
    "    def prior_kl(\n",
    "        self, latent_distribution: distributions.Distribution\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        The goal of this function is to calculate the Kullback leibler between the prior and the latent distribution for a given data sample $x$. In probabilistic notation, it should output $D_{KL}(prior || p(.|x)).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Prior kl not implemented\")\n",
    "\n",
    "    def r_sample(self, sample_shape: Tuple[int]) -> Tensor:\n",
    "        \"\"\"\n",
    "        The goal of this function is to return a reparametrized sample for the whole model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Sampling from model not implemented\")\n",
    "\n",
    "    def elbo(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :images: Torch tensor of size (n_batches, 1, 32, 32)\n",
    "        \"\"\"\n",
    "        #### YOUR CODE HERE.\n",
    "\n",
    "        return (\n",
    "            elbo,\n",
    "            likelihood_observation,\n",
    "            kl_prior,\n",
    "            latent_distribution,\n",
    "            data_given_latent,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[0].float()\n",
    "        elbo, lk, kl = self.elbo(images)[:3]\n",
    "\n",
    "        self.log(\"train/lk\", lk, prog_bar=True)\n",
    "        self.log(\"train/kl\", kl, prog_bar=True)\n",
    "        return -elbo\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        metrics = {}\n",
    "        images = batch[0].float()\n",
    "        elbo, lk, kl, latent_distribution, data_distribution = self.elbo(images)\n",
    "        fake_images = data_distribution.rsample((1,))[0]\n",
    "\n",
    "        metrics[\"val/elbo\"] = elbo\n",
    "        metrics[\"val/lk\"] = lk\n",
    "        metrics[\"val/kl\"] = kl\n",
    "        metrics[\"val/uRMSE\"] = linalg.vector_norm(\n",
    "            0.5 * (fake_images - images)\n",
    "        ).mean() / (images[0].numel() ** 0.5)\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            self.log(k, v, on_step=False, on_epoch=True, prog_bar=False, sync_dist=True)\n",
    "        if batch_idx == 0:\n",
    "            self.reconstructions_per_class = {\n",
    "                i: fake_images[i] for i in range(self.n_images_to_log)\n",
    "            }\n",
    "            self.original_per_class = {\n",
    "                i: images[i] for i in range(self.n_images_to_log)\n",
    "            }\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        norms = {\n",
    "            **{\n",
    "                f\"encoder/grad/{k}\": v.item()\n",
    "                for k, v in L.pytorch.utilities.grad_norm(\n",
    "                    self.encoder, norm_type=2\n",
    "                ).items()\n",
    "            },\n",
    "            **{\n",
    "                f\"decoder/grad/{k}\": v.item()\n",
    "                for k, v in L.pytorch.utilities.grad_norm(\n",
    "                    self.decoder, norm_type=2\n",
    "                ).items()\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.log_dict(norms)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        for cl in self.original_per_class:\n",
    "            self.logger.experiment.add_image(\n",
    "                f\"{cl}/rec_image\",\n",
    "                self.reconstructions_per_class[cl],\n",
    "                self.current_epoch,\n",
    "            )\n",
    "            self.logger.experiment.add_image(\n",
    "                f\"{cl}/or_image\", self.original_per_class[cl], self.current_epoch\n",
    "            )\n",
    "\n",
    "        self.original_per_class.clear()\n",
    "        self.reconstructions_per_class.clear()\n",
    "        z = self.prior.sample((8,))[:, 0]\n",
    "        prior_gen_images = self.decode(z).sample((1,))[0]\n",
    "        for i, img in enumerate(prior_gen_images):\n",
    "            self.logger.experiment.add_image(f\"gen_image/{i}\", img, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim_config[\"optimizer\"][\"type\"] == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                itertools.chain(self.encoder.parameters(), self.decoder.parameters()),\n",
    "                self.optim_config[\"optimizer\"][\"base_learning_rate\"],\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Adam implemented\")\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    factor=self.optim_config[\"lr_schedule\"][\"factor\"],\n",
    "                    mode=self.optim_config[\"lr_schedule\"][\"metric_mode\"],\n",
    "                ),\n",
    "                \"monitor\": self.optim_config[\"lr_schedule\"][\"metric_to_track\"],\n",
    "                \"frequency\": self.optim_config[\"lr_schedule\"][\"frequency\"],\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "class GaussGaussVAE(AbstractVAE):\n",
    "    def __init__(\n",
    "        self, prior_mean, prior_logvar, scale_max,  **kwargs\n",
    "    ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.dec_scale_max = scale_max\n",
    "        self.prior = None\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_logvar = prior_logvar\n",
    "\n",
    "        self.save_hyperparameters(\n",
    "            ignore=[\"encoder_class\", \"decoder_class\", \"prior_mean\", \"prior_logvar\"]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        #### YOUR CODE HERE\n",
    "        return None\n",
    "\n",
    "    def decode(self, latents):\n",
    "        #### YOUR CODE HERE\n",
    "        return None\n",
    "\n",
    "    def prior_kl(self, latent_distribution):\n",
    "        ### YOUR CODE HERE\n",
    "        return None\n",
    "\n",
    "    def rsample(self, sample_shape: Tuple[int]) -> Tensor:\n",
    "        latent_samples = self.prior.rsample(sample_shape).squeeze(len(sample_shape))\n",
    "        data_dist = self.decode(latent_samples)\n",
    "        return data_dist.rsample((1,))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are suggestions, you are welcomed to try different things.\n",
    "encoder_maker = lambda : nn.Sequential(\n",
    "    nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(64, 2, 1, stride=1, padding=0),\n",
    ")\n",
    "decoder_maker = lambda : nn.Sequential(\n",
    "    nn.ConvTranspose2d(1, 64, 3, output_padding=1, padding=1, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.ConvTranspose2d(64, 64, 3, output_padding=1, padding=1, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.ConvTranspose2d(64, 64, 3, output_padding=1, padding=1, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(64, 1, 1, stride=1),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = L.pytorch.loggers.TensorBoardLogger(\n",
    "    save_dir=\"data/VAE/logs\",\n",
    "    name=\"4x4_vae\",\n",
    "    version=f\"V1\",\n",
    ")\n",
    "checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    \"data/VAE/models\"\n",
    ")\n",
    "lr_monitor = L.pytorch.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=1,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    logger=logger,\n",
    "    max_epochs=10\n",
    ")\n",
    "latent_shape = (1, 4, 4)\n",
    "with trainer.init_module():\n",
    "    vae = GaussGaussVAE(\n",
    "        encoder_class=encoder_maker,\n",
    "        decoder_class=decoder_maker,\n",
    "        encoder_params={},\n",
    "        decoder_params={},\n",
    "        prior_mean=zeros((1,) + latent_shape),\n",
    "        prior_logvar=zeros((1,) + latent_shape),\n",
    "        optim_config={\"optimizer\": {\"base_learning_rate\": 1e-3, \"type\": \"Adam\"}, \"lr_schedule\": {\"factor\": .5, \"metric_mode\": \"min\", \"metric_to_track\": \"val/uRMSE\", \"frequency\": 1}},\n",
    "        variance_type=\"fixed\",\n",
    "        scale_max = 0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir data/VAE/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=vae,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=cv_loader,\n",
    "    ckpt_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959aeea",
   "metadata": {},
   "source": [
    "### Q2:\n",
    "\n",
    "Describe the optimization procedure implemented above for the training of the VAE (No need to comment on the ELBO, rather describe at optimizer level the choices that have been made.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ce065",
   "metadata": {},
   "source": [
    "## Evaluating the generative capabilities of the VAE\n",
    "In this section, we will focus on the evaluation of the model. \n",
    "\n",
    "### Q3: Write the code to generate 20 000 samples for the VAE and visualize a random subset of 10 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc29328",
   "metadata": {},
   "source": [
    "### Q4.1: Complete the code below to calculate an integral probability (semi-) metric and calculate it for the VAE case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ipm(samples_1, samples_2, ipf_member, max_iter, tol):\n",
    "    n_1 = samples_1.shape[0]\n",
    "    n_2 = samples_2.shape[0]\n",
    "    train_features = concatenate([samples_1[:n_1//2], samples_2[:n_2//2]], axis=0)\n",
    "    test_features = concatenate([samples_1[n_1//2:], samples_2[n_2//2:]], axis=0)\n",
    "    train_labels = concatenate([zeros((n_1//2,)), ones((n_2//2,))], axis=0)\n",
    "    test_labels = concatenate([zeros((n_1//2,)), ones((n_2//2,))], axis=0)\n",
    "\n",
    "    train_dataset = StackDataset(TensorDataset(train_features), TensorDataset(train_labels))\n",
    "    test_dataset = StackDataset(TensorDataset(test_features), TensorDataset(test_labels))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=1_000,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=1_000,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    optimizer = optim.Adam(ipf_member.parameters(), lr=1e-3, maximize=True)\n",
    "    pbar = tqdm(range(max_iter))\n",
    "    previous_per_epoch_loss = 1_00000\n",
    "    for it in pbar:\n",
    "        per_epoch_loss = []\n",
    "        for (i, (features, labels)) in enumerate(train_loader):\n",
    "            features, labels = features[0], labels[0]\n",
    "            # Insert your code HERE\n",
    "            loss=0\n",
    "            per_epoch_loss.append(loss.item())\n",
    "        per_epoch_loss = sum(per_epoch_loss) / len(per_epoch_loss)\n",
    "        if abs(previous_per_epoch_loss - per_epoch_loss) < tol:\n",
    "            break\n",
    "        previous_per_epoch_loss = per_epoch_loss\n",
    "        pbar.set_postfix({\"ipm\": per_epoch_loss})\n",
    "\n",
    "        \n",
    "    ipf_member.eval()\n",
    "    per_epoch_loss = []\n",
    "    for (i, (features, labels)) in enumerate(test_loader):\n",
    "        features, labels = features[0], labels[0]\n",
    "        # Insert your code HERE\n",
    "        loss=0\n",
    "        per_epoch_loss.append(loss.item())\n",
    "        \n",
    "    per_epoch_loss = sum(per_epoch_loss) / len(per_epoch_loss)\n",
    "    return per_epoch_loss, ipf_member\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipm = calculate_ipm(\n",
    "    samples_1=concatenate([test_dataset[i][0][None] for i in range(len(test_dataset))], axis=0),\n",
    "    samples_2=gen_samples,\n",
    "    ipf_member=nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.utils.parametrizations.weight_norm(nn.Linear(32 * 32, 1000, bias=False), \"weight\"),\n",
    "        nn.ReLU(),\n",
    "        nn.utils.parametrizations.weight_norm(nn.Linear(1000, 1000, bias=False), \"weight\"),\n",
    "        nn.ReLU(),\n",
    "        nn.utils.parametrizations.weight_norm(nn.Linear(1000, 1), \"weight\"),\n",
    "        nn.Sigmoid(),\n",
    "    ),\n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    ")[0]\n",
    "print(ipm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780f91f",
   "metadata": {},
   "source": [
    "### Q4.2 (Bonus): Why the usage of weight normalization here can be considered a reasonable choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e2ad0",
   "metadata": {},
   "source": [
    "## Looking further into the latent space:\n",
    "\n",
    "### Q5.1:  Write code to visualize the distributions of the projection of the cross validation dataset into the latent space as well as sample for the prior.\n",
    "\n",
    "### Q5.2: Why do the generative model is so poor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f42dbf",
   "metadata": {},
   "source": [
    "## Learning a diffusion model for the Latents\n",
    "\n",
    "In this section, following the discussion above from Q5, we focus on learning a denoiser over the Latent space.\n",
    "To do so, we will again use Lightning. \n",
    "\n",
    "### Q6: Complete the code below for the MultiLevelDenoiser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelDenoiser(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        denoiser_class: nn.Module,\n",
    "        denoiser_params: Dict[str, Any],\n",
    "        noise_scale_distribution: distributions.Distribution,\n",
    "        scaling_fun: Callable[[Tensor], Tensor],\n",
    "        optim_config,\n",
    "        n_images_to_log: int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.optim_config = optim_config\n",
    "        self.denoiser = denoiser_class(**denoiser_params)\n",
    "        self.noise_scale_distribution = noise_scale_distribution\n",
    "        self.n_images_to_log = n_images_to_log\n",
    "        self.scaling_fun = scaling_fun\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[0].float()\n",
    "        ### Your code here\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[0].float()\n",
    "        #### Your code here\n",
    "\n",
    "        self.log(\"validation/loss\", loss.item(), on_step=False, on_epoch=True, prog_bar=False, sync_dist=True)\n",
    "\n",
    "    \n",
    "        if batch_idx == 0:\n",
    "            self.reconstructions_per_class = {\n",
    "                i: denoised_images[i] for i in range(self.n_images_to_log)\n",
    "            }\n",
    "            self.original_per_class = {\n",
    "                i: images[i] for i in range(self.n_images_to_log)\n",
    "            }\n",
    "            self.noisy_per_class = {\n",
    "                 i: noised_images[i] for i in range(self.n_images_to_log)\n",
    "            }\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "\n",
    "        norms = {\n",
    "            **{\n",
    "                f\"denoiser/grad/{k}\": v.item()\n",
    "                for k, v in L.pytorch.utilities.grad_norm(\n",
    "                    self.denoiser, norm_type=2\n",
    "                ).items()\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.log_dict(norms)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        for cl in self.original_per_class:\n",
    "            or_image_range = (self.original_per_class[cl].min(), self.original_per_class[cl].max())\n",
    "            self.logger.experiment.add_image(\n",
    "                f\"{cl}/noisy_image\", (self.noisy_per_class[cl] - or_image_range[0]) / (or_image_range[1] - or_image_range[0])*1.5, self.current_epoch\n",
    "            )\n",
    "            self.logger.experiment.add_image(\n",
    "                f\"{cl}/rec_image\",\n",
    "                (self.reconstructions_per_class[cl] -  - or_image_range[0]) / (or_image_range[1] - or_image_range[0])*1.5,\n",
    "                self.current_epoch,\n",
    "            )\n",
    "            self.logger.experiment.add_image(\n",
    "                f\"{cl}/or_image\",\n",
    "                (self.original_per_class[cl] - or_image_range[0]) / (or_image_range[1] - or_image_range[0])*1.5,\n",
    "                 self.current_epoch\n",
    "            )\n",
    "            \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim_config[\"optimizer\"][\"type\"] == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                self.denoiser.parameters(),\n",
    "                self.optim_config[\"optimizer\"][\"base_learning_rate\"],\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Adam implemented\")\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    factor=self.optim_config[\"lr_schedule\"][\"factor\"],\n",
    "                    mode=self.optim_config[\"lr_schedule\"][\"metric_mode\"],\n",
    "                ),\n",
    "                \"monitor\": self.optim_config[\"lr_schedule\"][\"metric_to_track\"],\n",
    "                \"frequency\": self.optim_config[\"lr_schedule\"][\"frequency\"],\n",
    "            },\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c256eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, unet, sigma_data):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.sigma_data = sigma_data\n",
    "    \n",
    "    def forward(self, x, sigma):\n",
    "        # Preconditioning inspired by Karras et al (2022) Elucidating the desing space of Diffusion models.\n",
    "        sigma = sigma[:, None, None, None]\n",
    "        # Preconditioning weights.\n",
    "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
    "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.flatten().log() / 4\n",
    "\n",
    "        # Run the model.\n",
    "        x_in = (c_in * x)\n",
    "        F_x = self.unet(x_in, c_noise).sample\n",
    "        D_x = c_skip * x + c_out * F_x\n",
    "        return D_x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5785c49",
   "metadata": {},
   "source": [
    "One of the key features introduced in  Karras et al (2022) Elucidating the desing space of Diffusion models was the importance sampling of noise levels. We propose the following, but feel free to change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distributions.LogNormal(loc=-1., scale=1.6).sample((1000,)), bins=100, density=True)\n",
    "plt.xlim(0, 20)\n",
    "plt.title(\"Importance sampling of noise levels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141eb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = L.pytorch.loggers.TensorBoardLogger(\n",
    "    save_dir=\"data/Denoiser/logs\",\n",
    "    name=\"unet\",\n",
    "    version=f\"V1\",\n",
    ")\n",
    "checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    \"data/Denoiser/models\"\n",
    ")\n",
    "lr_monitor = L.pytorch.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=1,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    logger=logger,\n",
    "    max_epochs=10\n",
    ")\n",
    "latent_shape = (1, 4, 4)\n",
    "denoiser_params = {\"unet\": UNet2DModel(sample_size=[4, 4], in_channels=1, out_channels=1, down_block_types= ('DownBlock2D', 'AttnDownBlock2D'),  up_block_types=('AttnUpBlock2D', 'UpBlock2D'), block_out_channels=[128, 256]), \"sigma_data\": latents_train.std().item()}\n",
    "with trainer.init_module():\n",
    "    denoiser = MultiLevelDenoiser(\n",
    "        denoiser_class=Denoiser,\n",
    "        denoiser_params=denoiser_params,\n",
    "        noise_scale_distribution=distributions.LogNormal(loc=-1., scale=1.6),\n",
    "        scaling_fun=lambda x: 1/(x**2),\n",
    "        optim_config={\"optimizer\": {\"base_learning_rate\": 1e-3, \"type\": \"Adam\"}, \"lr_schedule\": {\"factor\": .5, \"metric_mode\": \"min\", \"metric_to_track\": \"validation/loss\", \"frequency\": 1}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train_loader = DataLoader(\n",
    "    TensorDataset(latents_train),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "latent_cv_loader = DataLoader(\n",
    "    TensorDataset(latents_cv),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=1,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir data/Denoiser/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=denoiser,\n",
    "    train_dataloaders=latent_train_loader,\n",
    "    val_dataloaders=latent_cv_loader,\n",
    "    ckpt_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c4c2f",
   "metadata": {},
   "source": [
    "## Evaluating the generative model\n",
    "\n",
    "Now that the model is trained, we focus on implementing an evaluation of its generative capabilities.\n",
    "### Q7: Complete the code in the function \"probability_flow_ode\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_flow_ode(\n",
    "    sigmas: Tensor,\n",
    "    initial_samples: Tensor,\n",
    "    denoiser_fn: Callable[[Tensor, Tensor], Tensor]\n",
    ") -> Tensor:\n",
    "    samples = initial_samples\n",
    "    for sigma_tm1, sigma_t in tqdm(zip(reversed(sigmas[:-1]), reversed(sigmas[1:]))):\n",
    "        ### YOUR CODE HERE\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_max = 80\n",
    "sigma_min = 0.002\n",
    "n_steps = 50\n",
    "rho = 3\n",
    "sigmas = (linspace(0, 1, n_steps) * (sigma_max ** (1/rho) - sigma_min ** (1/rho)) + sigma_min ** (1/rho))**rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_gen = 20_000\n",
    "batch_size = 1000\n",
    "vae.decoder.eval()\n",
    "vae.to(\"cuda:0\")\n",
    "denoiser.denoiser.eval()\n",
    "denoiser=denoiser.to(\"cuda:0\")\n",
    "gen_latents = []\n",
    "gen_samples = []\n",
    "for prior_samples in tqdm(randn((n_samples_gen// batch_size, batch_size) + latent_shape)*sigma_max):\n",
    "    with no_grad():\n",
    "        batch_gen_latents = probability_flow_ode(sigmas=sigmas, initial_samples=prior_samples.to(denoiser.device), denoiser_fn=lambda x, sigma: denoiser.denoiser(x, sigma*ones((x.shape[0],), device=x.device)))\n",
    "        batch_gen_samples = vae.decode(batch_gen_latents).base_dist.loc.cpu()\n",
    "        gen_samples.append(batch_gen_samples)\n",
    "        gen_latents.append(batch_gen_latents.cpu())\n",
    "\n",
    "gen_samples = concatenate(gen_samples, axis=0)\n",
    "gen_latents = concatenate(gen_latents, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_display = 10\n",
    "fig, axes = plt.subplots(1, n_display, figsize=(3*n_display, 3.5))\n",
    "for i, ax in zip(randint(0, gen_samples.shape[0], (n_display,)), axes):\n",
    "    ax.imshow(gen_samples[i, 0], vmin=0, vmax=1)\n",
    "    ax.set_axis_off()\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347ee7b",
   "metadata": {},
   "source": [
    "### Q8: Use the same tools as above to provide a diagnostic of the new model (Diffusion + VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fc003",
   "metadata": {},
   "source": [
    "### Q9: Comment the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
